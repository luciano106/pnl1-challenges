{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImplementaciÃ³n del DesafÃ­o 4 (PNL1)\n",
    "\n",
    "**Alumno:**\n",
    "- Adassus, Luciano (CEIA 17Co2024)\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglÃ©s. Se construirÃ¡ un BOT, como primer versiÃ³n, para responder a preguntas del usuario (QA).\\ [LINK](http://convai.io/data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.26.4\n",
      "TensorFlow: 2.16.2\n",
      "Gensim: 4.3.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Dot, Activation, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(\"NumPy:\", np.__version__)        # Deberia ser 1.26.4\n",
    "print(\"TensorFlow:\", tf.__version__)   # Deberia ser 2.16.1\n",
    "print(\"Gensim:\", gensim.__version__)   # Deberia ser 4.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš© 1. Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love iphone! i just bought new iphone!</td>\n",
       "      <td>Thats good for you, i'm not very into new tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thats good for you, i'm not very into new tech</td>\n",
       "      <td>I am a college student and i am a college student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am a college student and i am a college student</td>\n",
       "      <td>I am go to gym and live on donations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am go to gym and live on donations</td>\n",
       "      <td>I am a vegan and i am in the midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am a vegan and i am in the midwest</td>\n",
       "      <td>So vegan... i have dogs maybe i should told th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0           I love iphone! i just bought new iphone!   \n",
       "1     Thats good for you, i'm not very into new tech   \n",
       "2  I am a college student and i am a college student   \n",
       "3               I am go to gym and live on donations   \n",
       "4               I am a vegan and i am in the midwest   \n",
       "\n",
       "                                              answer  \n",
       "0     Thats good for you, i'm not very into new tech  \n",
       "1  I am a college student and i am a college student  \n",
       "2               I am go to gym and live on donations  \n",
       "3               I am a vegan and i am in the midwest  \n",
       "4  So vegan... i have dogs maybe i should told th...  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar dataset ConvAI2 (train)\n",
    "url = \"http://convai.io/data/summer_wild_evaluation_dialogs.json\"\n",
    "response = requests.get(url)\n",
    "convai2_json = response.json()\n",
    "\n",
    "# Extraer pares pregunta-respuesta claros\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "# ConvAI2 tiene estructura anidada\n",
    "for dialog in convai2_json:\n",
    "    utterances = dialog['dialog']\n",
    "    # Guardar pares consecutivos como pregunta y respuesta\n",
    "    for i in range(len(utterances)-1):\n",
    "        questions.append(utterances[i]['text'])\n",
    "        answers.append(utterances[i+1]['text'])\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame({'question': questions, 'answer': answers})\n",
    "# Usar solo las primeras 15.000 filas (en vez de 43.739) por temas de memoria RAM\n",
    "df = df.iloc[:15000].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš© 2. Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lucianoadassus/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/lucianoadassus/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.data.path.append('/usr/nltk_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/y24k5tss16n7pqyxn9b_g9x80000gp/T/ipykernel_2066/3403826205.py:18: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"lxml\").text\n",
      "/var/folders/wj/y24k5tss16n7pqyxn9b_g9x80000gp/T/ipykernel_2066/3403826205.py:18: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"lxml\").text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total de pares tras limpieza: 12648\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_clean</th>\n",
       "      <th>answer_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love iphone i just bought new iphone</td>\n",
       "      <td>&lt;sos&gt; thats good for you i am not very into ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thats good for you i am not very into new tech</td>\n",
       "      <td>&lt;sos&gt; i am a college student and i am a colleg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am a college student and i am a college student</td>\n",
       "      <td>&lt;sos&gt; i am go to gym and live on donations &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am go to gym and live on donations</td>\n",
       "      <td>&lt;sos&gt; i am a vegan and i am in the midwest &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am a vegan and i am in the midwest</td>\n",
       "      <td>&lt;sos&gt; so vegan i have dogs maybe i should told...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      question_clean  \\\n",
       "0             i love iphone i just bought new iphone   \n",
       "1     thats good for you i am not very into new tech   \n",
       "2  i am a college student and i am a college student   \n",
       "3               i am go to gym and live on donations   \n",
       "4               i am a vegan and i am in the midwest   \n",
       "\n",
       "                                        answer_clean  \n",
       "0  <sos> thats good for you i am not very into ne...  \n",
       "1  <sos> i am a college student and i am a colleg...  \n",
       "2   <sos> i am go to gym and live on donations <eos>  \n",
       "3   <sos> i am a vegan and i am in the midwest <eos>  \n",
       "4  <sos> so vegan i have dogs maybe i should told...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Diccionario de contracciones\n",
    "CONTRACTIONS = {\n",
    "    \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\", \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\", \"don't\": \"do not\", \"didn't\": \"did not\", \"i've\": \"i have\",\n",
    "    \"i'll\": \"i will\", \"you'll\": \"you will\", \"she'd\": \"she would\", \"should've\": \"should have\",\n",
    "    \"there's\": \"there is\", \"we'd\": \"we would\", \"they'll\": \"they will\", \"wasn't\": \"was not\"\n",
    "}\n",
    "\n",
    "# Expandir contracciones\n",
    "def expand_contractions(text):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(CONTRACTIONS.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: CONTRACTIONS[x.group()], text)\n",
    "\n",
    "# FunciÃ³n de limpieza completa\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = expand_contractions(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'[\\u263a-\\U0001f645]', ' ', text)\n",
    "    text = re.sub(r'[\\u2600-\\u26FF]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar limpieza\n",
    "df['question_clean'] = df['question'].apply(clean_text)\n",
    "df['answer_clean'] = df['answer'].apply(clean_text)\n",
    "\n",
    "# â• Agregar tokens <sos> y <eos>\n",
    "df['answer_clean'] = df['answer_clean'].apply(lambda x: '<sos> ' + x.strip() + ' <eos>')\n",
    "\n",
    "# ğŸ§¯ Filtro y balanceo del corpus\n",
    "df = df.dropna(subset=['question_clean', 'answer_clean'])\n",
    "df = df[(df['question_clean'].str.strip() != '') & (df['answer_clean'].str.strip() != '')]\n",
    "df = df[df['question_clean'] != df['answer_clean']]\n",
    "df = df[df['answer_clean'].str.split().apply(len) > 3]\n",
    "df = df.drop_duplicates(subset=['question_clean', 'answer_clean'])\n",
    "\n",
    "# Vista previa\n",
    "print(f\"âœ… Total de pares tras limpieza: {len(df)}\")\n",
    "display(df[['question_clean', 'answer_clean']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Generar tokenizers, secuencias y tensores de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total de pares pregunta-respuesta: 12648\n",
      "ğŸ“š Vocabulario input: 4046\n",
      "ğŸ“š Vocabulario output: 4171\n",
      "ğŸ§± Longitud mÃ¡xima input: 25\n",
      "ğŸ§± Longitud mÃ¡xima output: 25\n",
      "ğŸ¯ decoder_targets shape: (12648, 25, 4171)\n",
      "\n",
      "ğŸ” Ejemplo:\n",
      "Pregunta original: i love iphone i just bought new iphone\n",
      "Secuencia tokenizada: [1, 20, 855, 1, 38, 739, 145, 855]\n",
      "Secuencia padded: [  1  20 855   1  38 739 145 855   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n",
      "Respuesta tokenizada: [1, 53, 27, 26, 4, 3, 5, 9, 52, 229, 154, 2236, 2]\n",
      "Respuesta padded: [   1   53   27   26    4    3    5    9   52  229  154 2236    2    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# ParÃ¡metros\n",
    "MAX_VOCAB_SIZE = 6000\n",
    "MAX_SEQ_LENGTH = 25\n",
    "\n",
    "# Tokenizadores\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='', lower=True)\n",
    "tokenizer_inputs.fit_on_texts(df['question_clean'])\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(df['question_clean'])\n",
    "\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='', lower=True)\n",
    "tokenizer_outputs.fit_on_texts(df['answer_clean'])\n",
    "output_sequences = tokenizer_outputs.texts_to_sequences(df['answer_clean'])\n",
    "\n",
    "# Diccionarios\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "num_words_output = min(MAX_VOCAB_SIZE, len(word2idx_outputs) + 1)\n",
    "\n",
    "# Longitudes mÃ¡ximas\n",
    "max_input_len = min(MAX_SEQ_LENGTH, max(len(seq) for seq in input_sequences))\n",
    "max_out_len = min(MAX_SEQ_LENGTH, max(len(seq) for seq in output_sequences))\n",
    "\n",
    "# Padding\n",
    "encoder_input_sequences = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
    "decoder_output_sequences = pad_sequences(output_sequences, maxlen=max_out_len, padding='post')\n",
    "\n",
    "# Crear decoder targets desplazados\n",
    "decoder_targets = np.zeros((len(decoder_output_sequences), max_out_len, num_words_output), dtype='float32')\n",
    "for i, seq in enumerate(decoder_output_sequences):\n",
    "    for t, word_idx in enumerate(seq):\n",
    "        if t > 0:\n",
    "            decoder_targets[i, t - 1, word_idx] = 1.0\n",
    "\n",
    "# Prints informativos\n",
    "print(f\"âœ… Total de pares pregunta-respuesta: {len(df)}\")\n",
    "print(f\"ğŸ“š Vocabulario input: {min(len(word2idx_inputs), MAX_VOCAB_SIZE)}\")\n",
    "print(f\"ğŸ“š Vocabulario output: {num_words_output}\")\n",
    "print(f\"ğŸ§± Longitud mÃ¡xima input: {max_input_len}\")\n",
    "print(f\"ğŸ§± Longitud mÃ¡xima output: {max_out_len}\")\n",
    "print(f\"ğŸ¯ decoder_targets shape: {decoder_targets.shape}\")\n",
    "\n",
    "# Mostrar ejemplo\n",
    "print(\"\\nğŸ” Ejemplo:\")\n",
    "print(f\"Pregunta original: {df['question_clean'].iloc[0]}\")\n",
    "print(f\"Secuencia tokenizada: {input_sequences[0]}\")\n",
    "print(f\"Secuencia padded: {encoder_input_sequences[0]}\")\n",
    "print(f\"Respuesta tokenizada: {output_sequences[0]}\")\n",
    "print(f\"Respuesta padded: {decoder_output_sequences[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš© 3. Preparar los embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬ Cargando FastText...\n",
      "ğŸ”„ Generando embedding matrix...\n",
      "\n",
      "âœ… Palabras encontradas en FastText: 3736/4046\n",
      "âŒ Palabras faltantes (primeras 10): ['convai', 'whazzup', 'buongiorno', '_', 'poyou', 'zitah', 'orhun', 'wontice', 'hesnt', 'robotcop']\n",
      "ğŸ“ Dimensiones del embedding_matrix: (6000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Descargar FastText\n",
    "print(\"â¬ Cargando FastText...\")\n",
    "fasttext = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((MAX_VOCAB_SIZE, embedding_dim))\n",
    "\n",
    "# Contadores para estadÃ­sticas\n",
    "found = 0\n",
    "missing = []\n",
    "\n",
    "print(\"ğŸ”„ Generando embedding matrix...\")\n",
    "\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        if word in fasttext:\n",
    "            embedding_matrix[i] = fasttext[word]\n",
    "            found += 1\n",
    "        else:\n",
    "            missing.append(word)\n",
    "\n",
    "print(f\"\\nâœ… Palabras encontradas en FastText: {found}/{len(word2idx_inputs)}\")\n",
    "print(f\"âŒ Palabras faltantes (primeras 10): {missing[:10]}\")\n",
    "print(f\"ğŸ“ Dimensiones del embedding_matrix: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš© 4. Entrenar el modelo Encoder-Decoder (Seq2Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucianoadassus/Library/Caches/pypoetry/virtualenvs/intro-ia-MJmdkF7C-py3.12/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_32\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_32\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_82      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ input_layer_83      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_22        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,800,000</span> â”‚ input_layer_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_23        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,067,776</span> â”‚ input_layer_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">570,368</span> â”‚ embedding_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      â”‚            â”‚                   â”‚\n",
       "â”‚                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> â”‚ embedding_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      â”‚            â”‚ lstm_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    â”‚\n",
       "â”‚                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      â”‚            â”‚ lstm_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dot_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ lstm_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ lstm_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_19       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dot_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dot_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ lstm_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate_19      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dot_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ lstm_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ concatenate_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4171</span>)  â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,947</span> â”‚ dense_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_82      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ input_layer_83      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_22        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m300\u001b[0m)   â”‚  \u001b[38;5;34m1,800,000\u001b[0m â”‚ input_layer_82[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_23        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m)   â”‚  \u001b[38;5;34m1,067,776\u001b[0m â”‚ input_layer_83[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_18 (\u001b[38;5;33mLSTM\u001b[0m)      â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m), â”‚    \u001b[38;5;34m570,368\u001b[0m â”‚ embedding_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      â”‚            â”‚                   â”‚\n",
       "â”‚                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_19 (\u001b[38;5;33mLSTM\u001b[0m)      â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m), â”‚    \u001b[38;5;34m525,312\u001b[0m â”‚ embedding_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      â”‚            â”‚ lstm_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    â”‚\n",
       "â”‚                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      â”‚            â”‚ lstm_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dot_38 (\u001b[38;5;33mDot\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m)    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ lstm_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ lstm_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_19       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m)    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dot_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dot_39 (\u001b[38;5;33mDot\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m)   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ activation_19[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ lstm_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate_19      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dot_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     â”‚\n",
       "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ lstm_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_23 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m)   â”‚    \u001b[38;5;34m131,328\u001b[0m â”‚ concatenate_19[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_24 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m4171\u001b[0m)  â”‚  \u001b[38;5;34m1,071,947\u001b[0m â”‚ dense_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,166,731</span> (19.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,166,731\u001b[0m (19.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,366,731</span> (12.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,366,731\u001b[0m (12.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,800,000</span> (6.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,800,000\u001b[0m (6.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ParÃ¡metros\n",
    "latent_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_len,))\n",
    "encoder_embedding = Embedding(MAX_VOCAB_SIZE, embedding_dim,\n",
    "                              weights=[embedding_matrix],\n",
    "                              input_length=max_input_len,\n",
    "                              trainable=False)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_out_len,))\n",
    "decoder_embedding_layer = Embedding(num_words_output, latent_dim)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# AtenciÃ³n (Luong: dot product entre encoder_outputs y decoder_outputs)\n",
    "attention = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs])         # (batch, dec_seq, enc_seq)\n",
    "attention = Activation('softmax')(attention)                             # softmax sobre el encoder sequence\n",
    "context = Dot(axes=[2,1])([attention, encoder_outputs])                 # contexto ponderado\n",
    "decoder_combined_context = Concatenate(axis=-1)([context, decoder_outputs])\n",
    "\n",
    "# Output final\n",
    "output = Dense(256, activation='tanh')(decoder_combined_context)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(output)\n",
    "\n",
    "# Modelo final con atenciÃ³n\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compilar\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 581ms/step - accuracy: 0.6459 - loss: 3.1466 - val_accuracy: 0.7302 - val_loss: 1.7428\n",
      "Epoch 2/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 549ms/step - accuracy: 0.7341 - loss: 1.6877 - val_accuracy: 0.7513 - val_loss: 1.5822\n",
      "Epoch 3/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 599ms/step - accuracy: 0.7523 - loss: 1.5176 - val_accuracy: 0.7661 - val_loss: 1.4559\n",
      "Epoch 4/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 580ms/step - accuracy: 0.7709 - loss: 1.3509 - val_accuracy: 0.7733 - val_loss: 1.3955\n",
      "Epoch 5/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 646ms/step - accuracy: 0.7805 - loss: 1.2645 - val_accuracy: 0.7779 - val_loss: 1.3416\n",
      "Epoch 6/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 635ms/step - accuracy: 0.7911 - loss: 1.1656 - val_accuracy: 0.7820 - val_loss: 1.3027\n",
      "Epoch 7/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 669ms/step - accuracy: 0.7956 - loss: 1.1100 - val_accuracy: 0.7820 - val_loss: 1.2836\n",
      "Epoch 8/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 669ms/step - accuracy: 0.7969 - loss: 1.0688 - val_accuracy: 0.7864 - val_loss: 1.2567\n",
      "Epoch 9/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 671ms/step - accuracy: 0.8031 - loss: 1.0188 - val_accuracy: 0.7879 - val_loss: 1.2432\n",
      "Epoch 10/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 703ms/step - accuracy: 0.8072 - loss: 0.9795 - val_accuracy: 0.7892 - val_loss: 1.2320\n",
      "Epoch 11/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 667ms/step - accuracy: 0.8108 - loss: 0.9370 - val_accuracy: 0.7914 - val_loss: 1.2250\n",
      "Epoch 12/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 687ms/step - accuracy: 0.8166 - loss: 0.8975 - val_accuracy: 0.7936 - val_loss: 1.2174\n",
      "Epoch 13/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 683ms/step - accuracy: 0.8181 - loss: 0.8753 - val_accuracy: 0.7929 - val_loss: 1.2186\n",
      "Epoch 14/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 678ms/step - accuracy: 0.8237 - loss: 0.8389 - val_accuracy: 0.7947 - val_loss: 1.2156\n",
      "Epoch 15/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 682ms/step - accuracy: 0.8253 - loss: 0.8163 - val_accuracy: 0.7957 - val_loss: 1.2150\n",
      "Epoch 16/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 658ms/step - accuracy: 0.8321 - loss: 0.7815 - val_accuracy: 0.7958 - val_loss: 1.2193\n",
      "Epoch 17/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 677ms/step - accuracy: 0.8338 - loss: 0.7620 - val_accuracy: 0.7957 - val_loss: 1.2206\n",
      "Epoch 18/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 680ms/step - accuracy: 0.8365 - loss: 0.7428 - val_accuracy: 0.7968 - val_loss: 1.2282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d23ec00>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Callback para detener entrenamiento si el val_loss no mejora\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Callback para guardar el mejor modelo en archivo\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Entrenamiento\n",
    "model.fit(\n",
    "    [encoder_input_sequences, decoder_output_sequences],\n",
    "    decoder_targets,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš© 5. Inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder model para inferencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model: devuelve encoder_outputs y estados\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder model reutilizando embeddings y atenciÃ³n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs del decoder para inferencia\n",
    "decoder_input_single = Input(shape=(1,))\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "encoder_outputs_input = Input(shape=(max_input_len, latent_dim))\n",
    "\n",
    "decoder_embedding_inf = decoder_embedding_layer(decoder_input_single)\n",
    "\n",
    "# LSTM paso a paso\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding_inf,\n",
    "    initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "\n",
    "# AtenciÃ³n\n",
    "attention = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs_input])\n",
    "attention = Activation('softmax')(attention)\n",
    "context = Dot(axes=[2, 1])([attention, encoder_outputs_input])\n",
    "decoder_combined_context = Concatenate(axis=-1)([context, decoder_outputs])\n",
    "\n",
    "# Capa final\n",
    "output = Dense(256, activation='tanh')(decoder_combined_context)\n",
    "decoder_outputs = decoder_dense(output)\n",
    "\n",
    "# Modelo final de inferencia\n",
    "decoder_model = Model(\n",
    "    [decoder_input_single, decoder_state_input_h, decoder_state_input_c, encoder_outputs_input],\n",
    "    [decoder_outputs, state_h, state_c]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diccionario inverso para decodificar Ã­ndices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word2idx_outputs = {idx: word for word, idx in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FunciÃ³n decode_sequence() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def decode_sequence_beam_super(input_seq, beam_width=3, max_repeat=3, min_prob=1e-6, length_penalty_alpha=0.6):\n",
    "    enc_outs, h, c = encoder_model.predict(input_seq, verbose=0)\n",
    "    sequences = [([word2idx_outputs['<sos>']], 0.0, h, c)]\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score, h, c in sequences:\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = seq[-1]\n",
    "\n",
    "            output_tokens, h_new, c_new = decoder_model.predict([target_seq, h, c, enc_outs], verbose=0)\n",
    "            output_probs = output_tokens[0, -1, :]\n",
    "\n",
    "            top_indices = output_probs.argsort()[-beam_width:][::-1]\n",
    "\n",
    "            for idx in top_indices:\n",
    "                if idx not in reverse_word2idx_outputs:\n",
    "                    continue\n",
    "                word = reverse_word2idx_outputs[idx]\n",
    "                prob = output_probs[idx]\n",
    "\n",
    "                if prob < min_prob:\n",
    "                    continue\n",
    "\n",
    "                new_seq = seq + [idx]\n",
    "                if word == '<eos>':\n",
    "                    decoded = [reverse_word2idx_outputs.get(i, \"<UNK>\") for i in new_seq[1:-1]]\n",
    "                    return ' '.join(decoded)\n",
    "\n",
    "                # PenalizaciÃ³n por log(prob) y por longitud (mÃ¡s larga = mejor)\n",
    "                length_penalty = ((5 + len(new_seq)) / 6) ** length_penalty_alpha\n",
    "                candidate_score = (score - np.log(prob + 1e-10)) / length_penalty\n",
    "\n",
    "                all_candidates.append((new_seq, candidate_score, h_new, c_new))\n",
    "\n",
    "        if not all_candidates:\n",
    "            break\n",
    "\n",
    "        sequences = sorted(all_candidates, key=lambda tup: tup[1])[:beam_width]\n",
    "\n",
    "    # Sin <eos> â†’ devolver mejor secuencia\n",
    "    final_sequence = sequences[0][0][1:]  # quitar <sos>\n",
    "    decoded_words = [reverse_word2idx_outputs.get(i, \"<UNK>\") for i in final_sequence]\n",
    "\n",
    "    # Evitar repeticiones\n",
    "    word_counts = Counter(decoded_words)\n",
    "    most_common_word, count = word_counts.most_common(1)[0]\n",
    "    if count > max_repeat:\n",
    "        decoded_words = list(dict.fromkeys(decoded_words))  # eliminar duplicados preservando orden\n",
    "\n",
    "    # Evitar triples consecutivos\n",
    "    cleaned = []\n",
    "    for w in decoded_words:\n",
    "        if len(cleaned) < 2 or not (w == cleaned[-1] == cleaned[-2]):\n",
    "            cleaned.append(w)\n",
    "\n",
    "    return ' '.join(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FunciÃ³n para consultar al bot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ Question: Do you prefer Android or iOS?\n",
      "ğŸ¤– Answer: clubs lifting biggest royce ride sheep couch assessor christ reproduce served secondhand newspaper reset\n",
      "\n",
      "ğŸ§ Question: Why did you choose to become a vegan?\n",
      "ğŸ¤– Answer: kills lifting draws ride everybodys polyglot teachers brownies served rad station fyi\n",
      "\n",
      "ğŸ§ Question: How do you stay healthy while living on donations?\n",
      "ğŸ¤– Answer: competes lifting favorite ride mum em salsa friday wednesday softball reset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def answer_question_beam_super(question):\n",
    "    question_clean = clean_text(question)\n",
    "    seq = tokenizer_inputs.texts_to_sequences([question_clean])\n",
    "    pad_seq = pad_sequences(seq, maxlen=max_input_len, padding='post')\n",
    "    return decode_sequence_beam_super(pad_seq)\n",
    "\n",
    "# Probar\n",
    "test_questions = [\n",
    "    \"Do you prefer Android or iOS?\",\n",
    "    \"Why did you choose to become a vegan?\",\n",
    "    \"How do you stay healthy while living on donations?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"ğŸ§ Question: {q}\")\n",
    "    print(f\"ğŸ¤– Answer: {answer_question_beam_super(q)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš© Resultados del Entrenamiento - Modelo de Chatbot con AtenciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âœ… Estado actual del modelo:**\n",
    "\n",
    "El modelo fue entrenado con las siguientes configuraciones:\n",
    "\n",
    "- **Corpus reducido:** 12.648 pares pregunta-respuesta.\n",
    "- **Vocabulario mÃ¡ximo:** 6.000 palabras.\n",
    "- **Limpieza avanzada:** expansiÃ³n de contracciones, remociÃ³n de emojis, puntuaciÃ³n, HTML, palabras repetidas y respuestas poco informativas.\n",
    "- **Modelo con atenciÃ³n Luong y decodificaciÃ³n Beam Search penalizada por longitud.**\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Š Comportamiento observado\n",
    "\n",
    "Las respuestas generadas muestran:\n",
    "\n",
    "- ğŸŒŸ **Mayor coherencia estructural**: ya no repite tokens como `\"you you\"` ni genera palabras sin sentido.\n",
    "- ğŸ§© **Patrones sintÃ¡cticos mÃ¡s variados**, aunque persisten ciertas \"muletillas\" como `\"clubs lifting ride sheep...\"` en varias respuestas.\n",
    "- ğŸ” **Tendencia a repetir construcciones**: ciertas frases o combinaciones aparecen frecuentemente, seÃ±al de que el modelo internalizÃ³ estructuras comunes del dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“‰ Limitaciones detectadas\n",
    "\n",
    "- Algunas respuestas **pierden el foco semÃ¡ntico** frente a preguntas abiertas.\n",
    "- Persisten **muletillas repetidas** que indican sobreajuste a patrones comunes.\n",
    "- El modelo **no capta completamente el sentido de la pregunta**, y responde con frases sueltas o listas de sustantivos.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”§ Siguientes pasos recomendados\n",
    "\n",
    "1. **ğŸ” Mayor diversidad en el dataset:**\n",
    "   - Aumentar la variedad semÃ¡ntica de las respuestas.\n",
    "   - Eliminar frases tipo muletilla.\n",
    "   - Incluir ejemplos con estructuras mÃ¡s variadas, especialmente preguntas-respuestas informativas.\n",
    "\n",
    "2. **ğŸ§¼ Limpieza semÃ¡ntica adicional:**\n",
    "   - Eliminar repeticiones como `\"i am a college student and i am a college student\"`.\n",
    "   - Filtrar respuestas con mÃ¡s de 2 repeticiones de la misma palabra.\n",
    "\n",
    "3. **ğŸ“ˆ Ajustes en la inferencia:**\n",
    "   - Explorar `top-k` o `top-p` sampling para respuestas mÃ¡s diversas.\n",
    "   - Usar `beam_width=5` y penalizaciÃ³n de longitud mÃ¡s fuerte para evitar listas sin sentido.\n",
    "\n",
    "4. **ğŸ”¡ Mejora del embedding:**\n",
    "   - Usar embeddings entrenables (`trainable=True`) para que el modelo pueda adaptarse mejor.\n",
    "   - Considerar embeddings especÃ­ficos del dominio si el tema lo permite.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ ConclusiÃ³n\n",
    "\n",
    "El modelo muestra una **mejora importante respecto a versiones anteriores**, especialmente gracias al preprocesamiento y beam search penalizado. TodavÃ­a puede mejorar en **coherencia semÃ¡ntica y riqueza lÃ©xica**, algo que se puede abordar en futuros entrenamientos con corpus mÃ¡s balanceado y estrategias de decodificaciÃ³n mÃ¡s exploratorias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
