{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del Desafío 4 (PNL1)\n",
    "\n",
    "**Alumno:**\n",
    "- Adassus, Luciano (CEIA 17Co2024)\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT, como primer versión, para responder a preguntas del usuario (QA).\\ [LINK](http://convai.io/data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.26.4\n",
      "TensorFlow: 2.16.2\n",
      "Gensim: 4.3.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Dot, Activation, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(\"NumPy:\", np.__version__)        # Deberia ser 1.26.4\n",
    "print(\"TensorFlow:\", tf.__version__)   # Deberia ser 2.16.1\n",
    "print(\"Gensim:\", gensim.__version__)   # Deberia ser 4.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 1. Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love iphone! i just bought new iphone!</td>\n",
       "      <td>Thats good for you, i'm not very into new tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thats good for you, i'm not very into new tech</td>\n",
       "      <td>I am a college student and i am a college student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am a college student and i am a college student</td>\n",
       "      <td>I am go to gym and live on donations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am go to gym and live on donations</td>\n",
       "      <td>I am a vegan and i am in the midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am a vegan and i am in the midwest</td>\n",
       "      <td>So vegan... i have dogs maybe i should told th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0           I love iphone! i just bought new iphone!   \n",
       "1     Thats good for you, i'm not very into new tech   \n",
       "2  I am a college student and i am a college student   \n",
       "3               I am go to gym and live on donations   \n",
       "4               I am a vegan and i am in the midwest   \n",
       "\n",
       "                                              answer  \n",
       "0     Thats good for you, i'm not very into new tech  \n",
       "1  I am a college student and i am a college student  \n",
       "2               I am go to gym and live on donations  \n",
       "3               I am a vegan and i am in the midwest  \n",
       "4  So vegan... i have dogs maybe i should told th...  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar dataset ConvAI2 (train)\n",
    "url = \"http://convai.io/data/summer_wild_evaluation_dialogs.json\"\n",
    "response = requests.get(url)\n",
    "convai2_json = response.json()\n",
    "\n",
    "# Extraer pares pregunta-respuesta claros\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "# ConvAI2 tiene estructura anidada\n",
    "for dialog in convai2_json:\n",
    "    utterances = dialog['dialog']\n",
    "    # Guardar pares consecutivos como pregunta y respuesta\n",
    "    for i in range(len(utterances)-1):\n",
    "        questions.append(utterances[i]['text'])\n",
    "        answers.append(utterances[i+1]['text'])\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame({'question': questions, 'answer': answers})\n",
    "# Usar solo las primeras 15.000 filas (en vez de 43.739) por temas de memoria RAM\n",
    "df = df.iloc[:15000].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 2. Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lucianoadassus/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/lucianoadassus/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.data.path.append('/usr/nltk_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/y24k5tss16n7pqyxn9b_g9x80000gp/T/ipykernel_2066/3403826205.py:18: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"lxml\").text\n",
      "/var/folders/wj/y24k5tss16n7pqyxn9b_g9x80000gp/T/ipykernel_2066/3403826205.py:18: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"lxml\").text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total de pares tras limpieza: 12648\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_clean</th>\n",
       "      <th>answer_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love iphone i just bought new iphone</td>\n",
       "      <td>&lt;sos&gt; thats good for you i am not very into ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thats good for you i am not very into new tech</td>\n",
       "      <td>&lt;sos&gt; i am a college student and i am a colleg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am a college student and i am a college student</td>\n",
       "      <td>&lt;sos&gt; i am go to gym and live on donations &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am go to gym and live on donations</td>\n",
       "      <td>&lt;sos&gt; i am a vegan and i am in the midwest &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am a vegan and i am in the midwest</td>\n",
       "      <td>&lt;sos&gt; so vegan i have dogs maybe i should told...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      question_clean  \\\n",
       "0             i love iphone i just bought new iphone   \n",
       "1     thats good for you i am not very into new tech   \n",
       "2  i am a college student and i am a college student   \n",
       "3               i am go to gym and live on donations   \n",
       "4               i am a vegan and i am in the midwest   \n",
       "\n",
       "                                        answer_clean  \n",
       "0  <sos> thats good for you i am not very into ne...  \n",
       "1  <sos> i am a college student and i am a colleg...  \n",
       "2   <sos> i am go to gym and live on donations <eos>  \n",
       "3   <sos> i am a vegan and i am in the midwest <eos>  \n",
       "4  <sos> so vegan i have dogs maybe i should told...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Diccionario de contracciones\n",
    "CONTRACTIONS = {\n",
    "    \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\", \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\", \"don't\": \"do not\", \"didn't\": \"did not\", \"i've\": \"i have\",\n",
    "    \"i'll\": \"i will\", \"you'll\": \"you will\", \"she'd\": \"she would\", \"should've\": \"should have\",\n",
    "    \"there's\": \"there is\", \"we'd\": \"we would\", \"they'll\": \"they will\", \"wasn't\": \"was not\"\n",
    "}\n",
    "\n",
    "# Expandir contracciones\n",
    "def expand_contractions(text):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(CONTRACTIONS.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: CONTRACTIONS[x.group()], text)\n",
    "\n",
    "# Función de limpieza completa\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = expand_contractions(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'[\\u263a-\\U0001f645]', ' ', text)\n",
    "    text = re.sub(r'[\\u2600-\\u26FF]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar limpieza\n",
    "df['question_clean'] = df['question'].apply(clean_text)\n",
    "df['answer_clean'] = df['answer'].apply(clean_text)\n",
    "\n",
    "# ➕ Agregar tokens <sos> y <eos>\n",
    "df['answer_clean'] = df['answer_clean'].apply(lambda x: '<sos> ' + x.strip() + ' <eos>')\n",
    "\n",
    "# 🧯 Filtro y balanceo del corpus\n",
    "df = df.dropna(subset=['question_clean', 'answer_clean'])\n",
    "df = df[(df['question_clean'].str.strip() != '') & (df['answer_clean'].str.strip() != '')]\n",
    "df = df[df['question_clean'] != df['answer_clean']]\n",
    "df = df[df['answer_clean'].str.split().apply(len) > 3]\n",
    "df = df.drop_duplicates(subset=['question_clean', 'answer_clean'])\n",
    "\n",
    "# Vista previa\n",
    "print(f\"✅ Total de pares tras limpieza: {len(df)}\")\n",
    "display(df[['question_clean', 'answer_clean']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Generar tokenizers, secuencias y tensores de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total de pares pregunta-respuesta: 12648\n",
      "📚 Vocabulario input: 4046\n",
      "📚 Vocabulario output: 4171\n",
      "🧱 Longitud máxima input: 25\n",
      "🧱 Longitud máxima output: 25\n",
      "🎯 decoder_targets shape: (12648, 25, 4171)\n",
      "\n",
      "🔎 Ejemplo:\n",
      "Pregunta original: i love iphone i just bought new iphone\n",
      "Secuencia tokenizada: [1, 20, 855, 1, 38, 739, 145, 855]\n",
      "Secuencia padded: [  1  20 855   1  38 739 145 855   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n",
      "Respuesta tokenizada: [1, 53, 27, 26, 4, 3, 5, 9, 52, 229, 154, 2236, 2]\n",
      "Respuesta padded: [   1   53   27   26    4    3    5    9   52  229  154 2236    2    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Parámetros\n",
    "MAX_VOCAB_SIZE = 6000\n",
    "MAX_SEQ_LENGTH = 25\n",
    "\n",
    "# Tokenizadores\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='', lower=True)\n",
    "tokenizer_inputs.fit_on_texts(df['question_clean'])\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(df['question_clean'])\n",
    "\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='', lower=True)\n",
    "tokenizer_outputs.fit_on_texts(df['answer_clean'])\n",
    "output_sequences = tokenizer_outputs.texts_to_sequences(df['answer_clean'])\n",
    "\n",
    "# Diccionarios\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "num_words_output = min(MAX_VOCAB_SIZE, len(word2idx_outputs) + 1)\n",
    "\n",
    "# Longitudes máximas\n",
    "max_input_len = min(MAX_SEQ_LENGTH, max(len(seq) for seq in input_sequences))\n",
    "max_out_len = min(MAX_SEQ_LENGTH, max(len(seq) for seq in output_sequences))\n",
    "\n",
    "# Padding\n",
    "encoder_input_sequences = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
    "decoder_output_sequences = pad_sequences(output_sequences, maxlen=max_out_len, padding='post')\n",
    "\n",
    "# Crear decoder targets desplazados\n",
    "decoder_targets = np.zeros((len(decoder_output_sequences), max_out_len, num_words_output), dtype='float32')\n",
    "for i, seq in enumerate(decoder_output_sequences):\n",
    "    for t, word_idx in enumerate(seq):\n",
    "        if t > 0:\n",
    "            decoder_targets[i, t - 1, word_idx] = 1.0\n",
    "\n",
    "# Prints informativos\n",
    "print(f\"✅ Total de pares pregunta-respuesta: {len(df)}\")\n",
    "print(f\"📚 Vocabulario input: {min(len(word2idx_inputs), MAX_VOCAB_SIZE)}\")\n",
    "print(f\"📚 Vocabulario output: {num_words_output}\")\n",
    "print(f\"🧱 Longitud máxima input: {max_input_len}\")\n",
    "print(f\"🧱 Longitud máxima output: {max_out_len}\")\n",
    "print(f\"🎯 decoder_targets shape: {decoder_targets.shape}\")\n",
    "\n",
    "# Mostrar ejemplo\n",
    "print(\"\\n🔎 Ejemplo:\")\n",
    "print(f\"Pregunta original: {df['question_clean'].iloc[0]}\")\n",
    "print(f\"Secuencia tokenizada: {input_sequences[0]}\")\n",
    "print(f\"Secuencia padded: {encoder_input_sequences[0]}\")\n",
    "print(f\"Respuesta tokenizada: {output_sequences[0]}\")\n",
    "print(f\"Respuesta padded: {decoder_output_sequences[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 3. Preparar los embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏬ Cargando FastText...\n",
      "🔄 Generando embedding matrix...\n",
      "\n",
      "✅ Palabras encontradas en FastText: 3736/4046\n",
      "❌ Palabras faltantes (primeras 10): ['convai', 'whazzup', 'buongiorno', '_', 'poyou', 'zitah', 'orhun', 'wontice', 'hesnt', 'robotcop']\n",
      "📐 Dimensiones del embedding_matrix: (6000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Descargar FastText\n",
    "print(\"⏬ Cargando FastText...\")\n",
    "fasttext = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((MAX_VOCAB_SIZE, embedding_dim))\n",
    "\n",
    "# Contadores para estadísticas\n",
    "found = 0\n",
    "missing = []\n",
    "\n",
    "print(\"🔄 Generando embedding matrix...\")\n",
    "\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        if word in fasttext:\n",
    "            embedding_matrix[i] = fasttext[word]\n",
    "            found += 1\n",
    "        else:\n",
    "            missing.append(word)\n",
    "\n",
    "print(f\"\\n✅ Palabras encontradas en FastText: {found}/{len(word2idx_inputs)}\")\n",
    "print(f\"❌ Palabras faltantes (primeras 10): {missing[:10]}\")\n",
    "print(f\"📐 Dimensiones del embedding_matrix: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 4. Entrenar el modelo Encoder-Decoder (Seq2Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucianoadassus/Library/Caches/pypoetry/virtualenvs/intro-ia-MJmdkF7C-py3.12/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_32\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_32\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_82      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_83      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_22        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,800,000</span> │ input_layer_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_23        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,067,776</span> │ input_layer_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">570,368</span> │ embedding_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ lstm_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_19       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ lstm_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_19      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ concatenate_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4171</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,947</span> │ dense_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_82      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_83      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_22        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │  \u001b[38;5;34m1,800,000\u001b[0m │ input_layer_82[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_23        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,067,776\u001b[0m │ input_layer_83[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_18 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m570,368\u001b[0m │ embedding_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_19 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m525,312\u001b[0m │ embedding_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_38 (\u001b[38;5;33mDot\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ lstm_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ lstm_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_19       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dot_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_39 (\u001b[38;5;33mDot\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ activation_19[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ lstm_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_19      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dot_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m131,328\u001b[0m │ concatenate_19[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m4171\u001b[0m)  │  \u001b[38;5;34m1,071,947\u001b[0m │ dense_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,166,731</span> (19.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,166,731\u001b[0m (19.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,366,731</span> (12.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,366,731\u001b[0m (12.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,800,000</span> (6.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,800,000\u001b[0m (6.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parámetros\n",
    "latent_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_len,))\n",
    "encoder_embedding = Embedding(MAX_VOCAB_SIZE, embedding_dim,\n",
    "                              weights=[embedding_matrix],\n",
    "                              input_length=max_input_len,\n",
    "                              trainable=False)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_out_len,))\n",
    "decoder_embedding_layer = Embedding(num_words_output, latent_dim)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Atención (Luong: dot product entre encoder_outputs y decoder_outputs)\n",
    "attention = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs])         # (batch, dec_seq, enc_seq)\n",
    "attention = Activation('softmax')(attention)                             # softmax sobre el encoder sequence\n",
    "context = Dot(axes=[2,1])([attention, encoder_outputs])                 # contexto ponderado\n",
    "decoder_combined_context = Concatenate(axis=-1)([context, decoder_outputs])\n",
    "\n",
    "# Output final\n",
    "output = Dense(256, activation='tanh')(decoder_combined_context)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(output)\n",
    "\n",
    "# Modelo final con atención\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compilar\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 581ms/step - accuracy: 0.6459 - loss: 3.1466 - val_accuracy: 0.7302 - val_loss: 1.7428\n",
      "Epoch 2/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 549ms/step - accuracy: 0.7341 - loss: 1.6877 - val_accuracy: 0.7513 - val_loss: 1.5822\n",
      "Epoch 3/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 599ms/step - accuracy: 0.7523 - loss: 1.5176 - val_accuracy: 0.7661 - val_loss: 1.4559\n",
      "Epoch 4/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 580ms/step - accuracy: 0.7709 - loss: 1.3509 - val_accuracy: 0.7733 - val_loss: 1.3955\n",
      "Epoch 5/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 646ms/step - accuracy: 0.7805 - loss: 1.2645 - val_accuracy: 0.7779 - val_loss: 1.3416\n",
      "Epoch 6/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 635ms/step - accuracy: 0.7911 - loss: 1.1656 - val_accuracy: 0.7820 - val_loss: 1.3027\n",
      "Epoch 7/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 669ms/step - accuracy: 0.7956 - loss: 1.1100 - val_accuracy: 0.7820 - val_loss: 1.2836\n",
      "Epoch 8/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 669ms/step - accuracy: 0.7969 - loss: 1.0688 - val_accuracy: 0.7864 - val_loss: 1.2567\n",
      "Epoch 9/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 671ms/step - accuracy: 0.8031 - loss: 1.0188 - val_accuracy: 0.7879 - val_loss: 1.2432\n",
      "Epoch 10/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 703ms/step - accuracy: 0.8072 - loss: 0.9795 - val_accuracy: 0.7892 - val_loss: 1.2320\n",
      "Epoch 11/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 667ms/step - accuracy: 0.8108 - loss: 0.9370 - val_accuracy: 0.7914 - val_loss: 1.2250\n",
      "Epoch 12/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 687ms/step - accuracy: 0.8166 - loss: 0.8975 - val_accuracy: 0.7936 - val_loss: 1.2174\n",
      "Epoch 13/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 683ms/step - accuracy: 0.8181 - loss: 0.8753 - val_accuracy: 0.7929 - val_loss: 1.2186\n",
      "Epoch 14/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 678ms/step - accuracy: 0.8237 - loss: 0.8389 - val_accuracy: 0.7947 - val_loss: 1.2156\n",
      "Epoch 15/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 682ms/step - accuracy: 0.8253 - loss: 0.8163 - val_accuracy: 0.7957 - val_loss: 1.2150\n",
      "Epoch 16/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 658ms/step - accuracy: 0.8321 - loss: 0.7815 - val_accuracy: 0.7958 - val_loss: 1.2193\n",
      "Epoch 17/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 677ms/step - accuracy: 0.8338 - loss: 0.7620 - val_accuracy: 0.7957 - val_loss: 1.2206\n",
      "Epoch 18/30\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 680ms/step - accuracy: 0.8365 - loss: 0.7428 - val_accuracy: 0.7968 - val_loss: 1.2282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d23ec00>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Callback para detener entrenamiento si el val_loss no mejora\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Callback para guardar el mejor modelo en archivo\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Entrenamiento\n",
    "model.fit(\n",
    "    [encoder_input_sequences, decoder_output_sequences],\n",
    "    decoder_targets,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 5. Inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder model para inferencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model: devuelve encoder_outputs y estados\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder model reutilizando embeddings y atención:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs del decoder para inferencia\n",
    "decoder_input_single = Input(shape=(1,))\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "encoder_outputs_input = Input(shape=(max_input_len, latent_dim))\n",
    "\n",
    "decoder_embedding_inf = decoder_embedding_layer(decoder_input_single)\n",
    "\n",
    "# LSTM paso a paso\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding_inf,\n",
    "    initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "\n",
    "# Atención\n",
    "attention = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs_input])\n",
    "attention = Activation('softmax')(attention)\n",
    "context = Dot(axes=[2, 1])([attention, encoder_outputs_input])\n",
    "decoder_combined_context = Concatenate(axis=-1)([context, decoder_outputs])\n",
    "\n",
    "# Capa final\n",
    "output = Dense(256, activation='tanh')(decoder_combined_context)\n",
    "decoder_outputs = decoder_dense(output)\n",
    "\n",
    "# Modelo final de inferencia\n",
    "decoder_model = Model(\n",
    "    [decoder_input_single, decoder_state_input_h, decoder_state_input_c, encoder_outputs_input],\n",
    "    [decoder_outputs, state_h, state_c]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diccionario inverso para decodificar índices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word2idx_outputs = {idx: word for word, idx in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función decode_sequence() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def decode_sequence_beam_super(input_seq, beam_width=3, max_repeat=3, min_prob=1e-6, length_penalty_alpha=0.6):\n",
    "    enc_outs, h, c = encoder_model.predict(input_seq, verbose=0)\n",
    "    sequences = [([word2idx_outputs['<sos>']], 0.0, h, c)]\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score, h, c in sequences:\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = seq[-1]\n",
    "\n",
    "            output_tokens, h_new, c_new = decoder_model.predict([target_seq, h, c, enc_outs], verbose=0)\n",
    "            output_probs = output_tokens[0, -1, :]\n",
    "\n",
    "            top_indices = output_probs.argsort()[-beam_width:][::-1]\n",
    "\n",
    "            for idx in top_indices:\n",
    "                if idx not in reverse_word2idx_outputs:\n",
    "                    continue\n",
    "                word = reverse_word2idx_outputs[idx]\n",
    "                prob = output_probs[idx]\n",
    "\n",
    "                if prob < min_prob:\n",
    "                    continue\n",
    "\n",
    "                new_seq = seq + [idx]\n",
    "                if word == '<eos>':\n",
    "                    decoded = [reverse_word2idx_outputs.get(i, \"<UNK>\") for i in new_seq[1:-1]]\n",
    "                    return ' '.join(decoded)\n",
    "\n",
    "                # Penalización por log(prob) y por longitud (más larga = mejor)\n",
    "                length_penalty = ((5 + len(new_seq)) / 6) ** length_penalty_alpha\n",
    "                candidate_score = (score - np.log(prob + 1e-10)) / length_penalty\n",
    "\n",
    "                all_candidates.append((new_seq, candidate_score, h_new, c_new))\n",
    "\n",
    "        if not all_candidates:\n",
    "            break\n",
    "\n",
    "        sequences = sorted(all_candidates, key=lambda tup: tup[1])[:beam_width]\n",
    "\n",
    "    # Sin <eos> → devolver mejor secuencia\n",
    "    final_sequence = sequences[0][0][1:]  # quitar <sos>\n",
    "    decoded_words = [reverse_word2idx_outputs.get(i, \"<UNK>\") for i in final_sequence]\n",
    "\n",
    "    # Evitar repeticiones\n",
    "    word_counts = Counter(decoded_words)\n",
    "    most_common_word, count = word_counts.most_common(1)[0]\n",
    "    if count > max_repeat:\n",
    "        decoded_words = list(dict.fromkeys(decoded_words))  # eliminar duplicados preservando orden\n",
    "\n",
    "    # Evitar triples consecutivos\n",
    "    cleaned = []\n",
    "    for w in decoded_words:\n",
    "        if len(cleaned) < 2 or not (w == cleaned[-1] == cleaned[-2]):\n",
    "            cleaned.append(w)\n",
    "\n",
    "    return ' '.join(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para consultar al bot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧍 Question: Do you prefer Android or iOS?\n",
      "🤖 Answer: clubs lifting biggest royce ride sheep couch assessor christ reproduce served secondhand newspaper reset\n",
      "\n",
      "🧍 Question: Why did you choose to become a vegan?\n",
      "🤖 Answer: kills lifting draws ride everybodys polyglot teachers brownies served rad station fyi\n",
      "\n",
      "🧍 Question: How do you stay healthy while living on donations?\n",
      "🤖 Answer: competes lifting favorite ride mum em salsa friday wednesday softball reset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def answer_question_beam_super(question):\n",
    "    question_clean = clean_text(question)\n",
    "    seq = tokenizer_inputs.texts_to_sequences([question_clean])\n",
    "    pad_seq = pad_sequences(seq, maxlen=max_input_len, padding='post')\n",
    "    return decode_sequence_beam_super(pad_seq)\n",
    "\n",
    "# Probar\n",
    "test_questions = [\n",
    "    \"Do you prefer Android or iOS?\",\n",
    "    \"Why did you choose to become a vegan?\",\n",
    "    \"How do you stay healthy while living on donations?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"🧍 Question: {q}\")\n",
    "    print(f\"🤖 Answer: {answer_question_beam_super(q)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 Resultados del Entrenamiento - Modelo de Chatbot con Atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**✅ Estado actual del modelo:**\n",
    "\n",
    "El modelo fue entrenado con las siguientes configuraciones:\n",
    "\n",
    "- **Corpus reducido:** 12.648 pares pregunta-respuesta.\n",
    "- **Vocabulario máximo:** 6.000 palabras.\n",
    "- **Limpieza avanzada:** expansión de contracciones, remoción de emojis, puntuación, HTML, palabras repetidas y respuestas poco informativas.\n",
    "- **Modelo con atención Luong y decodificación Beam Search penalizada por longitud.**\n",
    "\n",
    "---\n",
    "\n",
    "#### 📊 Comportamiento observado\n",
    "\n",
    "Las respuestas generadas muestran:\n",
    "\n",
    "- 🌟 **Mayor coherencia estructural**: ya no repite tokens como `\"you you\"` ni genera palabras sin sentido.\n",
    "- 🧩 **Patrones sintácticos más variados**, aunque persisten ciertas \"muletillas\" como `\"clubs lifting ride sheep...\"` en varias respuestas.\n",
    "- 🔁 **Tendencia a repetir construcciones**: ciertas frases o combinaciones aparecen frecuentemente, señal de que el modelo internalizó estructuras comunes del dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📉 Limitaciones detectadas\n",
    "\n",
    "- Algunas respuestas **pierden el foco semántico** frente a preguntas abiertas.\n",
    "- Persisten **muletillas repetidas** que indican sobreajuste a patrones comunes.\n",
    "- El modelo **no capta completamente el sentido de la pregunta**, y responde con frases sueltas o listas de sustantivos.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔧 Siguientes pasos recomendados\n",
    "\n",
    "1. **🔁 Mayor diversidad en el dataset:**\n",
    "   - Aumentar la variedad semántica de las respuestas.\n",
    "   - Eliminar frases tipo muletilla.\n",
    "   - Incluir ejemplos con estructuras más variadas, especialmente preguntas-respuestas informativas.\n",
    "\n",
    "2. **🧼 Limpieza semántica adicional:**\n",
    "   - Eliminar repeticiones como `\"i am a college student and i am a college student\"`.\n",
    "   - Filtrar respuestas con más de 2 repeticiones de la misma palabra.\n",
    "\n",
    "3. **📈 Ajustes en la inferencia:**\n",
    "   - Explorar `top-k` o `top-p` sampling para respuestas más diversas.\n",
    "   - Usar `beam_width=5` y penalización de longitud más fuerte para evitar listas sin sentido.\n",
    "\n",
    "4. **🔡 Mejora del embedding:**\n",
    "   - Usar embeddings entrenables (`trainable=True`) para que el modelo pueda adaptarse mejor.\n",
    "   - Considerar embeddings específicos del dominio si el tema lo permite.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🏁 Conclusión\n",
    "\n",
    "El modelo muestra una **mejora importante respecto a versiones anteriores**, especialmente gracias al preprocesamiento y beam search penalizado. Todavía puede mejorar en **coherencia semántica y riqueza léxica**, algo que se puede abordar en futuros entrenamientos con corpus más balanceado y estrategias de decodificación más exploratorias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
